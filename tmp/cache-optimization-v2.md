<!-- d:\dev\SimpleTraderV3\tmp\cache-optimization-v2.md -->
<!-- template=design version=89c6c82d created=2026-01-26T10:30:00Z updated= --># Cache Optimization Strategy
**Status:** DRAFT  
**Version:** 1.0  
**Created:** 2026-01-26  
**Last Updated:** 2026-01-26  
**Implementation Phase:** Phase 2.1 - Architecture
---

## Purpose

Define caching strategy to improve system performance and reduce API rate limit violations

## Scope

**In Scope:**
- Market data caching layer between API client and trading logic
- Cache invalidation and TTL management
- Performance metrics and monitoring

**Out of Scope:**
- Historical data caching (handled by separate archival system)
- User session caching (managed by authentication layer)
- Database query caching (deferred to Phase 3.0)

## Prerequisites

Read these first:
1. Redis cluster configured with pub/sub enabled
2. Performance baseline metrics collected

---

## 1. Context & Requirements

### 1.1. Problem Statement

The current system experiences performance degradation when handling concurrent price data requests. Market data API rate limits are frequently exceeded, and redundant calculations slow down decision-making processes.

### 1.2. Requirements

**Functional:**
- [ ] Cache market price data for configurable TTL (5-60 seconds)
- [ ] Support concurrent read access without blocking
- [ ] Invalidate stale data automatically
- [ ] Provide cache hit/miss metrics

**Non-Functional:**
- [ ] 99.9% availability for cache service
- [ ] Sub-millisecond cache lookup latency
- [ ] Memory footprint < 500MB for 10k symbols
- [ ] Thread-safe operations

### 1.3. ConstraintsMust integrate with existing Redis infrastructure. Cannot introduce dependencies on external caching services. Must maintain backward compatibility with current API endpoints.
---

## 2. Design Options

### 2.1. Option A: In-Memory Cache with TTL

Implement thread-safe in-process cache using Python's `cachetools` library with TTL-based expiration.

**Pros:**
- âœ… Zero network latency for cache hits
- âœ… Simple implementation with proven library
- âœ… No additional infrastructure required

**Cons:**
- âŒ Cache not shared across worker processes
- âŒ Memory duplication per worker
- âŒ No persistence across restarts

### 2.2. Option B: Redis-Based Distributed Cache

Leverage existing Redis cluster for centralized caching with pub/sub invalidation.

**Pros:**
- âœ… Shared cache across all workers
- âœ… Persistent storage with configurable eviction
- âœ… Built-in atomic operations and clustering

**Cons:**
- âŒ Network roundtrip adds 1-3ms latency
- âŒ Increased Redis load and complexity
- âŒ Requires serialization/deserialization overhead

### 2.3. Option C: Hybrid Two-Tier Cache

L1 in-memory cache (100ms TTL) backed by L2 Redis cache (60s TTL) with write-through strategy.

**Pros:**
- âœ… Combines low latency of local cache with consistency of distributed cache
- âœ… Reduces Redis load by 80-90% via L1 hits
- âœ… Graceful degradation if Redis unavailable

**Cons:**
- âŒ Most complex implementation
- âŒ Potential consistency issues during invalidation
- âŒ Higher memory usage per worker

---

## 3. Chosen Design

**Decision:** Implement Hybrid Two-Tier Cache (Option C)

**Rationale:** The hybrid approach provides optimal balance between performance and consistency. L1 cache eliminates network latency for frequently accessed symbols while L2 ensures cache coherence across workers. Performance modeling shows 95% of requests will hit L1 cache, achieving sub-millisecond latency while reducing Redis load by 87%.

### 3.1. Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| L1 TTL set to 100ms | Balances staleness risk with performance gains; market data updates typically occur every 200-500ms |
| Use Redis pub/sub for L1 invalidation | Ensures all workers invalidate L1 within 10ms of data updates, maintaining consistency |
| Implement circuit breaker for Redis | Degrades gracefully to L1-only mode if Redis becomes unavailable, maintaining service continuity |
---

## 4. Open Questions

| Question | Options | Status |
|----------|---------|--------|
| Should we implement cache warming on startup to reduce cold-start latency? | - | ðŸ”´ Open |
| What metrics should trigger automatic L1 TTL adjustment? | - | ðŸ”´ Open |
| How to handle cache consistency during deployment rollouts? | - | ðŸ”´ Open |

## Related Documentation
- **[docs/architecture/data-flow.md][related-1]**
- **[docs/reference/redis-configuration.md][related-2]**

<!-- Link definitions -->

[related-1]: docs/architecture/data-flow.md
[related-2]: docs/reference/redis-configuration.md

---

## Version History

| Version | Date | Author | Changes |
|---------|------|---------|--------|---------|
| 1.0 | 2026-01-26 | Agent | Initial draft |